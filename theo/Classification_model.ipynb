{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Import Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Data Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess the dataset\n",
    "def load_data(csv_file, img_dir, transform=None):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    images, physical_features, labels = [], [], []\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        img_id = str(data.iloc[idx]['id'])\n",
    "        img_path = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if transform:\n",
    "            image = transform(image)\n",
    "        \n",
    "        stable_height = int(data.iloc[idx]['stable_height']) - 1  # zero-based class index\n",
    "        block_type = data.iloc[idx]['type']\n",
    "        cam_angle = data.iloc[idx]['cam_angle']\n",
    "\n",
    "        images.append(image)\n",
    "        physical_features.append([block_type, cam_angle])\n",
    "        labels.append(stable_height)\n",
    "    \n",
    "    return images, torch.tensor(physical_features, dtype=torch.float32), torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Model Definition</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model\n",
    "class CustomResNetWithFeatures(nn.Module):\n",
    "    def __init__(self, num_types, num_classes):\n",
    "        super(CustomResNetWithFeatures, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)  # Using ResNet18\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 128)  # Modify ResNet output to 128 features\n",
    "\n",
    "        # Embedding for block type\n",
    "        self.type_embedding = nn.Embedding(num_types, 10)  # Embedding for block types\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(128 + 10 + 1, 64)  # Combine image features, block type embedding, and cam_angle\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # Final output for classification (logits for each class)\n",
    "\n",
    "    def forward(self, x, block_type, cam_angle):\n",
    "        x = self.resnet(x)  # Pass through ResNet\n",
    "        type_embed = self.type_embedding(block_type)  # Get block type embeddings\n",
    "        cam_angle = cam_angle.unsqueeze(1)  # Ensure cam_angle has correct shape\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat((x, type_embed, cam_angle), dim=1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(combined))  # Pass through the first fully connected layer\n",
    "        logits = self.fc2(x)  # Get final class logits\n",
    "        return logits\n",
    "\n",
    "# Update get_model function to return the model\n",
    "def get_model(num_types, num_classes):\n",
    "    return CustomResNetWithFeatures(num_types, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Training Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(csv_file_path, images_dir_path, num_types, num_classes=6, num_epochs=6, batch_size=16, model_save_path=\"model.pth\"):\n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load data\n",
    "    images, physical_features, labels = load_data(csv_file_path, images_dir_path, transform)\n",
    "    dataset = list(zip(images, physical_features, labels))\n",
    "    \n",
    "    # Use DataLoader without the need to stack tensors manually\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = get_model(num_types, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()  # For classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-4)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            images, physical_features, stable_heights = batch\n",
    "            images = images.to(device)  # No need to stack, already a tensor\n",
    "            block_type = (physical_features[:, 0] - 1).long().to(device)\n",
    "            cam_angle = physical_features[:, 1].to(device)\n",
    "            stable_heights = stable_heights.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images, block_type, cam_angle)\n",
    "            loss = criterion(logits, stable_heights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted_classes = torch.max(logits, 1)\n",
    "            running_correct += (predicted_classes == stable_heights).sum().item()\n",
    "\n",
    "        accuracy = running_correct / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader.dataset):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Save the model after every epoch\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved after Epoch {epoch+1} to {model_save_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Saving and Loading Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(model, model_path):\n",
    "    torch.save({'model_state_dict': model.state_dict()}, model_path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model_path, num_types):\n",
    "    model = get_model(num_types, num_classes=6)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. Generating Predictions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(test_images_dir, model_path, output_csv_file, num_types=2):\n",
    "    model = load_model(model_path, num_types)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for image_name in os.listdir(test_images_dir):\n",
    "        img_id = os.path.splitext(image_name)[0]\n",
    "        image_path = os.path.join(test_images_dir, image_name)\n",
    "        \n",
    "        image = load_data(image_path).to(device)\n",
    "        block_type = torch.tensor(1).to(device)\n",
    "        cam_angle = torch.tensor(0.0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image.unsqueeze(0), block_type.unsqueeze(0), cam_angle.unsqueeze(0))\n",
    "            predicted_class = output.argmax(dim=1).item() + 1\n",
    "\n",
    "        predictions.append({'id': img_id, 'stable_height': predicted_class})\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    pd.DataFrame(predictions).to_csv(output_csv_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7. Running the Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_save_path, num_types, num_classes):\n",
    "    # Initialize the model\n",
    "    model = get_model(num_types, num_classes)\n",
    "    \n",
    "    # Load the saved model state dict\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()  # Set the model to evaluation mode (important for inference)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/theo/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 0.0904, Accuracy: 0.3764\n",
      "Model saved after Epoch 1 to model.pth\n",
      "Epoch [2/6], Loss: 0.0753, Accuracy: 0.5102\n",
      "Model saved after Epoch 2 to model.pth\n",
      "Epoch [3/6], Loss: 0.0661, Accuracy: 0.5854\n",
      "Model saved after Epoch 3 to model.pth\n",
      "Epoch [4/6], Loss: 0.0553, Accuracy: 0.6589\n",
      "Model saved after Epoch 4 to model.pth\n",
      "Epoch [5/6], Loss: 0.0441, Accuracy: 0.7371\n",
      "Model saved after Epoch 5 to model.pth\n",
      "Epoch [6/6], Loss: 0.0325, Accuracy: 0.8143\n",
      "Model saved after Epoch 6 to model.pth\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = '../COMP90086_2024_Project_train/train.csv'  # Path to your training CSV\n",
    "images_dir_path = '../COMP90086_2024_Project_train/train'  # Path to your training images\n",
    "model = train_model(csv_file_path, images_dir_path, num_types=2, num_classes=6, model_save_path='model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomResNetWithFeatures(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      "  (type_embedding): Embedding(2, 10)\n",
      "  (fc1): Linear(in_features=139, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/theo/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_20287/1111035268.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    }
   ],
   "source": [
    "model_ = load_model('model.pth', num_types=2, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    95    706   2854 ... 998916 999235 999651]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../COMP90086_2024_Project_test/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     60\u001b[0m test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../COMP90086_2024_Project_test/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, skip_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Assuming the first column is 'id'\u001b[39;00m\n\u001b[1;32m     62\u001b[0m test_image_data \u001b[38;5;241m=\u001b[39m load_images_from_folder(folder_path, ids)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Preprocess test images\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder_path, ids, image_size=(224, 224)):\n",
    "    images = []\n",
    "    for id in ids:\n",
    "        filename = str(int(id)) + '.jpg'\n",
    "\n",
    "        # Load the image using OpenCV\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Resize the image to the target size (224, 224)\n",
    "        img = cv2.resize(img, image_size)\n",
    "\n",
    "        # Convert BGR (OpenCV default) to RGB if needed (TensorFlow models use RGB)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize pixel values to the range [0, 1]\n",
    "        img_array = img / 255.0\n",
    "\n",
    "        images.append(img_array)\n",
    "\n",
    "    # Convert the list of images to a NumPy array with shape (n, 224, 224, 3)\n",
    "    return np.array(images)\n",
    "\n",
    "folder_path = '../COMP90086_2024_Project_test/test'\n",
    "test = np.genfromtxt('../COMP90086_2024_Project_test/test.csv', delimiter=',', skip_header=1)\n",
    "ids = np.array(test, dtype=int)\n",
    "test_image_data = load_images_from_folder(folder_path, ids)\n",
    "\n",
    "# Predict stable height on the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
